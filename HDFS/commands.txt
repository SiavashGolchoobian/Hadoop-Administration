hadoop version										#show hadoop version
hdfs dfs -mkdir <directory path>					#make directory on hdfs
hdfs dfs -ls [-R] [-h] <directory path>				#"-h" means huan readable, "-R" means show all sub dirctories recursively
hdfs dfs -put <src> <hdfsdest>						#Send file from any source outside of dest hdfs to our dest hdfs storage (this command is advanced vs copyFromLocal)
hdfs dfs -get <hdfssrc> <dest>						#get/download file from hdfs to any detination.
hdfs dfs -copyFromLocal <localsrc> <hdfsdest>		#Copy file from local client machine to hdfs storage (This command is limited and simpler vs PUT)
hdfs dfs -copyToLocal <hdfssource> <localdest>		#get/download file from hdfs to local machine
hdfs dfs -moveFromLocal <localsrc> <hdfsdest>		#move file from local client machine to hdfs storage and delete that file from local
hdfs dfs -cat <filepath>							#read and show <filepath> file content
hdfs dfs -mv <hdfssrc> <hdfsdest>					#move or rename file in hdfs
hdfs dfs -cp <hdfssrc> <hdfsdest>					#copy file/folder inside hdfs
hdfs dfs -rm [-r] <filepath>						#remove file/folder from hdfs. use "-r" to remove recursively all subdirectories
hdfs dfs -expunge									#delete hadoop trash (recyclebin), because hadoop by default does not delete files permanently and move it to trash
hdfs dfs -chmod <mode> <filepath>					#change file or folder permissions, <mode> is like linux rules (rwx)
hdfs dfs -chown <user>:<group> <filepath>			#change file or folder ownership like linux.
hdfs dfs -chgrp <group> <filepath>					#change file or folder group ownership only, like linux(this command is simpler version of chown).
hdfs dfs -setrep <n> <filepath>						#change replication factor of <filepath> to <n>
hdfs dfs -head <filepath>							#read top head lines of <filepath>
hdfs dfs -tail <filepath>							#read last bottom lines of <filepath>
hdfs fsck <filepath> -files -blocks -locations		#file system check command show block information of file in data nodes

---------------------------------------
Example01: Copy income.csv file from hdfs to current directory in local client
	hdfs dfs -get /data/incomes/income.csv .
	OR
	hdfs dfs -copyFromLocal /data/incomes/income.csv .
	
Example02: Copy all files from income directory on hdfs to current directory in local client
	hdfs dfs -get /data/incomes/* .
	OR
	hdfs dfs -copyFromLocal /data/incomes/* .

Exmaple03: Count total lines of file content from hdfs (in this command we cat file from hdfs storgae and pipe it to local linux wc command to count it's lines)
	hdfs dfs -cat /data/incomes/income.csv | wc -l